{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgfk6OezAZ5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AlexNet(object):\n",
        "    \"\"\"Implementation of the AlexNet.\"\"\"\n",
        "\n",
        "    def __init__(self, x, keep_prob, num_classes, skip_layer,\n",
        "                 weights_path='DEFAULT'):\n",
        "        \"\"\"Create the graph of the AlexNet model.\n",
        "        Args:\n",
        "            x: Placeholder for the input tensor.\n",
        "            keep_prob: Dropout probability.\n",
        "            num_classes: Number of classes in the dataset.\n",
        "            skip_layer: List of names of the layer, that get trained from\n",
        "                scratch\n",
        "            weights_path: Complete path to the pretrained weight file, if it\n",
        "                isn't in the same folder as this code\n",
        "        \"\"\"\n",
        "        # Parse input arguments into class variables\n",
        "        self.X = x\n",
        "        self.NUM_CLASSES = num_classes\n",
        "        self.KEEP_PROB = keep_prob\n",
        "        self.SKIP_LAYER = skip_layer\n",
        "\n",
        "        if weights_path == 'DEFAULT':\n",
        "            self.WEIGHTS_PATH = 'savedFile/bvlc_alexnet.npy'\n",
        "        else:\n",
        "            self.WEIGHTS_PATH = weights_path\n",
        "\n",
        "        # Call the create function to build the computational graph of AlexNet\n",
        "        self.create()\n",
        "\n",
        "    def create(self):\n",
        "        \"\"\"Create the network graph.\"\"\"\n",
        "        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
        "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
        "        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\n",
        "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
        "        \n",
        "        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
        "        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n",
        "        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\n",
        "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
        "        \n",
        "        # 3rd Layer: Conv (w ReLu)\n",
        "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n",
        "\n",
        "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
        "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n",
        "\n",
        "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
        "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n",
        "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
        "\n",
        "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
        "        flattened = tf.reshape(pool5, [-1, 6*6*256])\n",
        "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
        "        #dropout6 = dropout(fc6, self.KEEP_PROB)\n",
        "\n",
        "        # 7th Layer: FC (w ReLu) -> Dropout\n",
        "        fc7 = fc(fc6, 4096, 4096, name='fc7')\n",
        "        #dropout7 = dropout(fc7, self.KEEP_PROB)\n",
        "\n",
        "        # 8th Layer: FC and return unscaled activations\n",
        "        self.fc8 = fc(fc7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\n",
        "\n",
        "    def load_initial_weights(self, session):\n",
        "        \"\"\"Load weights from file into network.\n",
        "        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n",
        "        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\n",
        "        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\n",
        "        'biases') we need a special load function\n",
        "        \"\"\"\n",
        "        # Load the weights into memory\n",
        "        weights_dict = np.load(self.WEIGHTS_PATH,allow_pickle=True, encoding='bytes').item()\n",
        "        \n",
        "        \n",
        "        # Loop over all layer names stored in the weights dict\n",
        "        for op_name in weights_dict:\n",
        "            print(op_name)\n",
        "            # Check if layer should be trained from scratch\n",
        "            if op_name not in self.SKIP_LAYER:\n",
        "\n",
        "                with tf.variable_scope(op_name, reuse=True):\n",
        "\n",
        "                    # Assign weights/biases to their corresponding tf variable\n",
        "                    for data in weights_dict[op_name]:\n",
        "\n",
        "                        # Biases\n",
        "                        if len(data.shape) == 1:\n",
        "                            var = tf.get_variable('biases', trainable=False)\n",
        "                            session.run(var.assign(data))\n",
        "\n",
        "                        # Weights\n",
        "                        else:\n",
        "                            var = tf.get_variable('weights', trainable=False)\n",
        "                            session.run(var.assign(data))\n",
        "\n",
        "\n",
        "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n",
        "         padding='SAME', groups=1):\n",
        "    \"\"\"Create a convolution layer.\n",
        "    Adapted from: https://github.com/ethereon/caffe-tensorflow\n",
        "    \"\"\"\n",
        "    # Get number of input channels\n",
        "    input_channels = int(x.get_shape()[-1])\n",
        "\n",
        "    # Create lambda function for the convolution\n",
        "    convolve = lambda i, k: tf.nn.conv2d(i, k,\n",
        "                                         strides=[1, stride_y, stride_x, 1],\n",
        "                                         padding=padding)\n",
        "\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # Create tf variables for the weights and biases of the conv layer\n",
        "        weights = tf.get_variable('weights', shape=[filter_height,\n",
        "                                                    filter_width,\n",
        "                                                    input_channels/groups,\n",
        "                                                    num_filters])\n",
        "        biases = tf.get_variable('biases', shape=[num_filters])\n",
        "\n",
        "    if groups == 1:\n",
        "        conv = convolve(x, weights)\n",
        "\n",
        "    # In the cases of multiple groups, split inputs & weights and\n",
        "    else:\n",
        "        # Split input and weights and convolve them separately\n",
        "        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n",
        "        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n",
        "                                 value=weights)\n",
        "        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n",
        "\n",
        "        # Concat the convolved output together again\n",
        "        conv = tf.concat(axis=3, values=output_groups)\n",
        "\n",
        "    # Add biases\n",
        "    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n",
        "\n",
        "    # Apply relu function\n",
        "    relu = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "    return relu\n",
        "\n",
        "\n",
        "def fc(x, num_in, num_out, name, relu=True):\n",
        "    \"\"\"Create a fully connected layer.\"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "\n",
        "        # Create tf variables for the weights and biases\n",
        "        weights = tf.get_variable('weights', shape=[num_in, num_out],\n",
        "                                  trainable=True)\n",
        "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
        "\n",
        "        # Matrix multiply weights and inputs and add bias\n",
        "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
        "\n",
        "    if relu:\n",
        "        # Apply ReLu non linearity\n",
        "        relu = tf.nn.relu(act)\n",
        "        return relu\n",
        "    else:\n",
        "        return act\n",
        "\n",
        "\n",
        "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n",
        "             padding='SAME'):\n",
        "    \"\"\"Create a max pooling layer.\"\"\"\n",
        "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
        "                          strides=[1, stride_y, stride_x, 1],\n",
        "                          padding=padding, name=name)\n",
        "\n",
        "\n",
        "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
        "    \"\"\"Create a local response normalization layer.\"\"\"\n",
        "    return tf.nn.local_response_normalization(x, depth_radius=radius,\n",
        "                                              alpha=alpha, beta=beta,\n",
        "                                              bias=bias, name=name)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#from tensorflow.contrib.data import Dataset\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework.ops import convert_to_tensor\n",
        "\n",
        "IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32)\n",
        "\n",
        "\n",
        "class ImageDataGenerator(object):\n",
        "    \"\"\"Wrapper class around the new Tensorflows dataset pipeline.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, txt_file, mode, batch_size, num_classes, shuffle=True,\n",
        "                 buffer_size=1000):\n",
        "        \"\"\"Create a new ImageDataGenerator.\n",
        "\n",
        "\n",
        "        Args:\n",
        "            txt_file: Path to the text file.\n",
        "            mode: Either 'training' or 'validation'. Depending on this value,\n",
        "                different parsing functions will be used.\n",
        "            batch_size: Number of images per batch.\n",
        "            num_classes: Number of classes in the dataset.\n",
        "            shuffle: Wether or not to shuffle the data in the dataset and the\n",
        "                initial file list.\n",
        "            buffer_size: Number of images used as buffer for TensorFlows\n",
        "                shuffling of the dataset.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an invalid mode is passed.\n",
        "\n",
        "        \"\"\"\n",
        "        self.txt_file = txt_file\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # retrieve the data from the text file\n",
        "        self._read_txt_file()\n",
        "\n",
        "        # number of samples in the dataset\n",
        "        self.data_size = len(self.labels)\n",
        "\n",
        "        # initial shuffling of the file and label lists (together!)\n",
        "        if shuffle:\n",
        "            self._shuffle_lists()\n",
        "\n",
        "        # convert lists to TF tensor\n",
        "        self.img_paths = convert_to_tensor(self.img_paths, dtype=dtypes.string)\n",
        "        self.labels = convert_to_tensor(self.labels, dtype=dtypes.int32)\n",
        "\n",
        "        # create dataset\n",
        "        data = tf.data.Dataset.from_tensor_slices((self.img_paths, self.labels))\n",
        "\n",
        "        # distinguish between train/infer. when calling the parsing functions\n",
        "        if mode == 'training':\n",
        "            data = data.map(self._parse_function_train,\n",
        "                      num_parallel_calls=100*batch_size)\n",
        "\n",
        "        elif mode == 'inference':\n",
        "            data = data.map(self._parse_function_inference,\n",
        "                      num_parallel_calls=100*batch_size)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode '%s'.\" % (mode))\n",
        "\n",
        "        # shuffle the first `buffer_size` elements of the dataset\n",
        "        if shuffle:\n",
        "            data = data.shuffle(buffer_size=buffer_size)\n",
        "\n",
        "        # create a new dataset with batches of images\n",
        "        data = data.batch(batch_size)\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def _read_txt_file(self):\n",
        "        \"\"\"Read the content of the text file and store it into lists.\"\"\"\n",
        "        self.img_paths = []\n",
        "        self.labels = []\n",
        "        with open(self.txt_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                items = line.split(' ')\n",
        "                self.img_paths.append(items[0])\n",
        "                self.labels.append(int(items[1]))\n",
        "\n",
        "    def _shuffle_lists(self):\n",
        "        \"\"\"Conjoined shuffling of the list of paths and labels.\"\"\"\n",
        "        path = self.img_paths\n",
        "        labels = self.labels\n",
        "        permutation = np.random.permutation(self.data_size)\n",
        "        self.img_paths = []\n",
        "        self.labels = []\n",
        "        for i in permutation:\n",
        "            self.img_paths.append(path[i])\n",
        "            self.labels.append(labels[i])\n",
        "\n",
        "    def _parse_function_train(self, filename, label):\n",
        "        \"\"\"Input parser for samples of the training set.\"\"\"\n",
        "        # convert label number into one-hot-encoding\n",
        "        one_hot = tf.one_hot(label, self.num_classes)\n",
        "\n",
        "        # load and preprocess the image\n",
        "        img_string = tf.read_file(filename)\n",
        "        img_decoded = tf.image.decode_jpeg(img_string, channels=3)\n",
        "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
        "        \"\"\"\n",
        "        Dataaugmentation comes here.\n",
        "        \"\"\"\n",
        "        #img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
        "\n",
        "        # RGB -> BGR\n",
        "        #img_bgr = img_centered[:, :, ::-1]\n",
        "\n",
        "        return img_resized, one_hot\n",
        "\n",
        "    def _parse_function_inference(self, filename, label):\n",
        "        \"\"\"Input parser for samples of the validation/test set.\"\"\"\n",
        "        # convert label number into one-hot-encoding\n",
        "        one_hot = tf.one_hot(label, self.num_classes)\n",
        "\n",
        "        # load and preprocess the image\n",
        "        img_string = tf.read_file(filename)\n",
        "        img_decoded = tf.image.decode_jpeg(img_string, channels=3)\n",
        "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
        "        #img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
        "\n",
        "        # RGB -> BGR\n",
        "        #img_bgr = img_centered[:, :, ::-1]\n",
        "\n",
        "        return img_resized, one_hot\n",
        "      \n",
        "\n",
        "\"\"\"Script to finetune AlexNet using Tensorflow.\n",
        "\n",
        "With this script you can finetune AlexNet as provided in the alexnet.py\n",
        "class on any given dataset. Specify the configuration settings at the\n",
        "beginning according to your problem.\n",
        "This script was written for TensorFlow >= version 1.2rc0 and comes with a blog\n",
        "post, which you can find here:\n",
        "\n",
        "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
        "\n",
        "Author: Frederik Kratzert\n",
        "contact: f.kratzert(at)gmail.com\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "#from tensorflow.contrib.data import Iterator\n",
        "#dataset = tf.data.Dataset.from_generator(real_gen, tf.float32)\n",
        "#iterator = iter(dataset)\n",
        "\n",
        "\"\"\"\n",
        "Configuration Part.\n",
        "\"\"\"\n",
        "# Path to the textfiles for the trainings and validation set\n",
        "train_file = 'numbers/train.txt'\n",
        "val_file = 'numbers/val.txt'\n",
        "\n",
        "# Learning params\n",
        "learning_rate = 0.00001\n",
        "num_epochs = 5\n",
        "batch_size = 1\n",
        "\n",
        "# Network params\n",
        "dropout_rate = 0.003\n",
        "num_classes = 5\n",
        "train_layers = ['fc8','fc7']\n",
        "\n",
        "# How often we want to write the tf.summary data to disk\n",
        "display_step = 5\n",
        "\n",
        "# Path for tf.summary.FileWriter and to store model checkpoints\n",
        "filewriter_path = \"savedFile\"\n",
        "checkpoint_path = \"savedFile\"\n",
        "\n",
        "\"\"\"\n",
        "Main Part of the finetuning Script.\n",
        "\"\"\"\n",
        "\n",
        "# Create parent path if it doesn't exist\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "    os.mkdir(checkpoint_path)\n",
        "\n",
        "# Place data loading and preprocessing on the cpu\n",
        "with tf.device('/cpu:0'):\n",
        "    tr_data = ImageDataGenerator(train_file,\n",
        "                                 mode='training',\n",
        "                                 batch_size=batch_size,\n",
        "                                 num_classes=num_classes,\n",
        "                                 shuffle=True)\n",
        "    val_data = ImageDataGenerator(val_file,\n",
        "                                  mode='inference',\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_classes=num_classes,\n",
        "                                  shuffle=False)\n",
        "\n",
        "    # create an reinitializable iterator given the dataset structure\n",
        "    iterator = tf.data.Iterator.from_structure(tr_data.data.output_types,\n",
        "                                       tr_data.data.output_shapes)\n",
        "    next_batch = iterator.get_next()\n",
        "\n",
        "# Ops for initializing the two different iterators\n",
        "training_init_op = iterator.make_initializer(tr_data.data)\n",
        "validation_init_op = iterator.make_initializer(val_data.data)\n",
        "\n",
        "# TF placeholder for graph input and output\n",
        "x = tf.placeholder(name=\"x\", dtype=tf.float32, shape=(batch_size, 227, 227, 3))\n",
        "y = tf.placeholder(name=\"y\", dtype=tf.float32,shape=(batch_size, num_classes))\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# Initialize model\n",
        "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
        "\n",
        "# Link variable to model output\n",
        "score = model.fc8\n",
        "out1 = tf.fake_quant_with_min_max_args(score, min=0., max=1., name=\"output1\")\n",
        "\n",
        "# List of trainable variables of the layers we want to train\n",
        "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
        "\n",
        "# Op for calculating the loss\n",
        "with tf.name_scope(\"cross_ent\"):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score,\n",
        "                                                                  labels=y))\n",
        "\n",
        "# Train op\n",
        "with tf.name_scope(\"train\"):\n",
        "    # Get gradients of all trainable variables\n",
        "    gradients = tf.gradients(loss, var_list)\n",
        "    gradients = list(zip(gradients, var_list))\n",
        "\n",
        "    # Create optimizer and apply gradient descent to the trainable variables\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
        "\n",
        "# Add gradients to summary\n",
        "#for gradient, var in gradients:\n",
        "    #tf.summary.histogram(var.name + '/gradient', gradient)\n",
        "\n",
        "# Add the variables we train to the summary\n",
        "#for var in var_list:\n",
        "    #tf.summary.histogram(var.name, var)\n",
        "\n",
        "# Add the loss to summary\n",
        "tf.summary.scalar('cross_entropy', loss)\n",
        "\n",
        "\n",
        "# Evaluation op: Accuracy of the model\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Add the accuracy to the summary\n",
        "tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "# Merge all summaries together\n",
        "merged_summary = tf.summary.merge_all()\n",
        "\n",
        "# Initialize the FileWriter\n",
        "writer = tf.summary.FileWriter(filewriter_path)\n",
        "\n",
        "# Initialize an saver for store model checkpoints\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Get the number of training/validation steps per epoch\n",
        "train_batches_per_epoch = int(np.floor(tr_data.data_size/batch_size))\n",
        "val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
        "print(\"val_batches_per_epoch\")\n",
        "print(val_batches_per_epoch)\n",
        "\n",
        "# Start Tensorflow session\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Initialize all variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Add the model graph to TensorBoard\n",
        "    writer.add_graph(sess.graph)\n",
        "\n",
        "    # Load the pretrained weights into the non-trainable layer\n",
        "    model.load_initial_weights(sess)\n",
        "\n",
        "    #print(\"{} Start training...\".format(datetime.now()))\n",
        "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),filewriter_path))\n",
        "                                                      \n",
        "\n",
        "    # Loop over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
        "\n",
        "        # Initialize iterator with the training dataset\n",
        "        sess.run(training_init_op)\n",
        "\n",
        "        for step in range(train_batches_per_epoch):\n",
        "\n",
        "            # get next batch of data\n",
        "            img_batch, label_batch = sess.run(next_batch)\n",
        "\n",
        "            # And run the training op\n",
        "            sess.run(train_op, feed_dict={x: img_batch,\n",
        "                                          y: label_batch,\n",
        "                                          keep_prob: dropout_rate})\n",
        "\n",
        "            # Generate summary with the current batch of data and write to file\n",
        "            if step % display_step == 0:\n",
        "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
        "                                                        y: label_batch,\n",
        "                                                        keep_prob: 1.})\n",
        "\n",
        "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
        "\n",
        "        # Validate the model on the entire validation set\n",
        "        print(\"{} Start validation\".format(datetime.now()))\n",
        "        sess.run(validation_init_op)\n",
        "        test_acc = 0.\n",
        "        test_count = 0\n",
        "        for _ in range(val_batches_per_epoch):\n",
        "\n",
        "            img_batch, label_batch = sess.run(next_batch)\n",
        "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
        "                                                y: label_batch,\n",
        "                                                keep_prob: 1.})\n",
        "            test_acc += acc\n",
        "            test_count += 1\n",
        "        test_acc /= test_count\n",
        "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
        "                                                       test_acc))\n",
        "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
        "\n",
        "        # save checkpoint of the model\n",
        "        converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [x], [score])\n",
        "        converter.post_training_quantize=True\n",
        "        checkpoint_name = os.path.join(checkpoint_path,'model_epoch'+str(epoch+1)+'.ckpt')\n",
        "                                       \n",
        "        save_path = saver.save(sess, checkpoint_name)\n",
        "        tf.train.write_graph(sess.graph.as_graph_def(), '.', 'savedFile/saved_model.pbtxt', as_text=False)\n",
        "        converter.inference_type = tf.contrib.lite.constants.FLOAT\n",
        "        input_arrays = converter.get_input_arrays()\n",
        "        converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev\n",
        "        tflite_model = converter.convert()\n",
        "        open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
        "        #print(\"{} Model checkpoint saved at {}\".format(datetime.now(),checkpoint_name))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}